{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "963909a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Neural_Nets.ThermoNet.Development.ThermoNetTorch import ThermoNet, ThermoLossFunc, ThermoDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from torch.optim import Rprop\n",
    "from Data_Handling.SGTEHandler.Development.SGTEHandler import SGTEHandler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16a84ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(net: ThermoNet, dataloader, loss_func, optimizer):\n",
    "    epoch_losses = np.zeros([len(dataloader), ])\n",
    "\n",
    "    for i, (temp, g, s, h, c) in enumerate(dataloader):\n",
    "        temp = temp.unsqueeze(-1)\n",
    "\n",
    "        # Forward pass\n",
    "        gibbs_energy, entropy, enthalpy, heat_cap = net(temp, temp, temp, temp)\n",
    "\n",
    "        scale = 10000\n",
    "        gibbs_energy, entropy, enthalpy, heat_cap = gibbs_energy/scale, entropy/scale, enthalpy/scale, heat_cap/scale\n",
    "        g, s, h, c = g/scale, s/scale, h/scale, c/scale\n",
    "\n",
    "        # Get the loss\n",
    "        loss = loss_func(gibbs_energy.float(), g.float(), entropy.float(), s.float(), enthalpy.float(), h.float(),\n",
    "                         heat_cap.float(), c.float())\n",
    "\n",
    "        # Backward pass\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 100)\n",
    "        optimizer.step()\n",
    "        epoch_losses[i] = loss\n",
    "\n",
    "    mean_epoch_loss = epoch_losses.mean()\n",
    "    print('Mean epoch loss: ', mean_epoch_loss)\n",
    "    return mean_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "71c09938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, element, phase):\n",
    "    # Hyperparameters\n",
    "    n_epochs = 500\n",
    "    lr = 0.05\n",
    "    batch_size = 16\n",
    "    std_thresh = 0.05\n",
    "\n",
    "    # Data\n",
    "    dataset = ThermoDataset(element, phase)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = Rprop(net.parameters(), lr=lr)\n",
    "    loss_func = ThermoLossFunc()\n",
    "\n",
    "    losses = []\n",
    "    \n",
    "    # Keep track of epoch where learning rate was reduced last\n",
    "    lr_reduced_last = 0\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        print('-----\\nEpoch %i:\\n' % i)\n",
    "        loss = epoch(net, dataloader, loss_func, optimizer)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Adapt learning rate if standard deviation over the last 10 epochs is below a threshold\n",
    "        if np.array(losses[-10:]).std() < std_thresh and (i - lr_reduced_last) >= 10:\n",
    "            print('Learning rate halfed! \\n')\n",
    "            lr_reduced_last = i\n",
    "            lr /= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6294eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphic_evaluation(net, element, phase):\n",
    "    start_temp = 200\n",
    "    end_temp = 2000\n",
    "    temp_range = torch.tensor(list(range(start_temp, end_temp)), dtype=torch.float64).unsqueeze(-1)\n",
    "\n",
    "    # Get true data\n",
    "    sgte_handler = SGTEHandler(element)\n",
    "    sgte_handler.evaluate_equations(start_temp, end_temp, 1e5, plot=False, phases=phase, entropy=True, enthalpy=True,\n",
    "                                    heat_capacity=True)\n",
    "    data = sgte_handler.equation_result_data\n",
    "\n",
    "    # Get values\n",
    "    temp = torch.tensor(data['Temperature'], dtype=torch.float64)\n",
    "    gibbs = torch.tensor(data.iloc[:, 1])\n",
    "    entropy = torch.tensor(data.iloc[:, 2])\n",
    "    enthalpy = torch.tensor(data.iloc[:, 3])\n",
    "    heat_cap = torch.tensor(data.iloc[:, 4])\n",
    "    \n",
    "    if start_temp < temp.min():\n",
    "        temp_range = torch.tensor(list(range(int(temp.min()), end_temp)), dtype=torch.float64).unsqueeze(-1)\n",
    "\n",
    "    gibbs_pred, entropy_pred, enthalpy_pred, heat_cap_pred = net(temp_range, temp_range, temp_range, temp_range)\n",
    "\n",
    "    gibbs_pred = gibbs_pred.detach()\n",
    "    entropy_pred = entropy_pred.detach()\n",
    "    enthalpy_pred = enthalpy_pred.detach()\n",
    "    heat_cap_pred = heat_cap_pred.detach()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(temp, gibbs)\n",
    "    plt.grid()\n",
    "    plt.scatter(temp_range, gibbs_pred, s=0.3, c='red')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(temp, entropy)\n",
    "    plt.grid()\n",
    "    plt.scatter(temp_range, entropy_pred, s=0.3, c='red')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(temp, enthalpy)\n",
    "    plt.grid()\n",
    "    plt.scatter(temp_range, enthalpy_pred, s=0.3, c='red')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(temp, heat_cap)\n",
    "    plt.grid()\n",
    "    plt.scatter(temp_range, heat_cap_pred, s=0.3, c='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c4da86df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fe successfully selected!\n",
      "\n",
      "-----\n",
      "Epoch 0:\n",
      "\n",
      "Mean epoch loss:  7.957581000907399\n",
      "-----\n",
      "Epoch 1:\n",
      "\n",
      "Mean epoch loss:  4.749310217171072\n",
      "-----\n",
      "Epoch 2:\n",
      "\n",
      "Mean epoch loss:  4.545609405107587\n",
      "-----\n",
      "Epoch 3:\n",
      "\n",
      "Mean epoch loss:  4.517665435220594\n",
      "-----\n",
      "Epoch 4:\n",
      "\n",
      "Mean epoch loss:  4.553784858400577\n",
      "-----\n",
      "Epoch 5:\n",
      "\n",
      "Mean epoch loss:  4.463663602543768\n",
      "-----\n",
      "Epoch 6:\n",
      "\n",
      "Mean epoch loss:  4.427689371822036\n",
      "-----\n",
      "Epoch 7:\n",
      "\n",
      "Mean epoch loss:  4.383941699411268\n",
      "-----\n",
      "Epoch 8:\n",
      "\n",
      "Mean epoch loss:  4.286846684518261\n",
      "-----\n",
      "Epoch 9:\n",
      "\n",
      "Mean epoch loss:  4.281068288277242\n",
      "-----\n",
      "Epoch 10:\n",
      "\n",
      "Mean epoch loss:  4.287426429374196\n",
      "-----\n",
      "Epoch 11:\n",
      "\n",
      "Mean epoch loss:  4.321660741467342\n",
      "-----\n",
      "Epoch 12:\n",
      "\n",
      "Mean epoch loss:  4.27499183093276\n",
      "-----\n",
      "Epoch 13:\n",
      "\n",
      "Mean epoch loss:  4.241568217767734\n",
      "-----\n",
      "Epoch 14:\n",
      "\n",
      "Mean epoch loss:  4.3110162833026635\n",
      "-----\n",
      "Epoch 15:\n",
      "\n",
      "Mean epoch loss:  4.218645505816023\n",
      "-----\n",
      "Epoch 16:\n",
      "\n",
      "Mean epoch loss:  4.246716746660036\n",
      "Learning rate halfed! \n",
      "\n",
      "-----\n",
      "Epoch 17:\n",
      "\n",
      "Mean epoch loss:  4.278587428208824\n",
      "-----\n",
      "Epoch 18:\n",
      "\n",
      "Mean epoch loss:  4.1725166512426926\n",
      "-----\n",
      "Epoch 19:\n",
      "\n",
      "Mean epoch loss:  4.166278591779905\n",
      "-----\n",
      "Epoch 20:\n",
      "\n",
      "Mean epoch loss:  4.185613447260634\n",
      "-----\n",
      "Epoch 21:\n",
      "\n",
      "Mean epoch loss:  4.155302896677891\n",
      "-----\n",
      "Epoch 22:\n",
      "\n",
      "Mean epoch loss:  4.135575365797382\n",
      "-----\n",
      "Epoch 23:\n",
      "\n",
      "Mean epoch loss:  4.174051719291188\n",
      "-----\n",
      "Epoch 24:\n",
      "\n",
      "Mean epoch loss:  4.098260705716142\n",
      "-----\n",
      "Epoch 25:\n",
      "\n",
      "Mean epoch loss:  4.111743818933719\n",
      "-----\n",
      "Epoch 26:\n",
      "\n",
      "Mean epoch loss:  4.108736635368561\n",
      "Learning rate halfed! \n",
      "\n",
      "-----\n",
      "Epoch 27:\n",
      "\n",
      "Mean epoch loss:  4.082578669084567\n",
      "-----\n",
      "Epoch 28:\n",
      "\n",
      "Mean epoch loss:  4.144661623740864\n",
      "-----\n",
      "Epoch 29:\n",
      "\n",
      "Mean epoch loss:  4.189645292602967\n",
      "-----\n",
      "Epoch 30:\n",
      "\n",
      "Mean epoch loss:  4.118718381239989\n",
      "-----\n",
      "Epoch 31:\n",
      "\n",
      "Mean epoch loss:  4.089459616447163\n",
      "-----\n",
      "Epoch 32:\n",
      "\n",
      "Mean epoch loss:  4.077718701318045\n",
      "-----\n",
      "Epoch 33:\n",
      "\n",
      "Mean epoch loss:  4.0844510648852195\n",
      "-----\n",
      "Epoch 34:\n",
      "\n",
      "Mean epoch loss:  4.130118296525189\n",
      "-----\n",
      "Epoch 35:\n",
      "\n",
      "Mean epoch loss:  4.087099542127591\n",
      "-----\n",
      "Epoch 36:\n",
      "\n",
      "Mean epoch loss:  4.070225653247299\n",
      "Learning rate halfed! \n",
      "\n",
      "-----\n",
      "Epoch 37:\n",
      "\n",
      "Mean epoch loss:  4.123852788844955\n",
      "-----\n",
      "Epoch 38:\n",
      "\n",
      "Mean epoch loss:  4.113500580609402\n",
      "-----\n",
      "Epoch 39:\n",
      "\n",
      "Mean epoch loss:  4.026330863203958\n",
      "-----\n",
      "Epoch 40:\n",
      "\n",
      "Mean epoch loss:  4.092248392996387\n",
      "-----\n",
      "Epoch 41:\n",
      "\n",
      "Mean epoch loss:  4.1126547996128835\n",
      "-----\n",
      "Epoch 42:\n",
      "\n",
      "Mean epoch loss:  4.057754126664634\n",
      "-----\n",
      "Epoch 43:\n",
      "\n",
      "Mean epoch loss:  4.1220617706530565\n",
      "-----\n",
      "Epoch 44:\n",
      "\n",
      "Mean epoch loss:  4.103880296243686\n",
      "-----\n",
      "Epoch 45:\n",
      "\n",
      "Mean epoch loss:  4.036263038064832\n",
      "-----\n",
      "Epoch 46:\n",
      "\n",
      "Mean epoch loss:  4.054190343785509\n",
      "Learning rate halfed! \n",
      "\n",
      "-----\n",
      "Epoch 47:\n",
      "\n",
      "Mean epoch loss:  4.0732158433611145\n",
      "-----\n",
      "Epoch 48:\n",
      "\n",
      "Mean epoch loss:  4.022926594609412\n",
      "-----\n",
      "Epoch 49:\n",
      "\n",
      "Mean epoch loss:  4.030529572584919\n",
      "-----\n",
      "Epoch 50:\n",
      "\n",
      "Mean epoch loss:  4.028093529638843\n",
      "-----\n",
      "Epoch 51:\n",
      "\n",
      "Mean epoch loss:  4.001299151750368\n",
      "-----\n",
      "Epoch 52:\n",
      "\n",
      "Mean epoch loss:  4.035126182520501\n",
      "-----\n",
      "Epoch 53:\n",
      "\n",
      "Mean epoch loss:  4.018025947508411\n",
      "-----\n",
      "Epoch 54:\n",
      "\n",
      "Mean epoch loss:  3.9786613867661664\n",
      "-----\n",
      "Epoch 55:\n",
      "\n",
      "Mean epoch loss:  4.019467803919427\n",
      "-----\n",
      "Epoch 56:\n",
      "\n",
      "Mean epoch loss:  4.0100995948381515\n",
      "Learning rate halfed! \n",
      "\n",
      "-----\n",
      "Epoch 57:\n",
      "\n",
      "Mean epoch loss:  4.01891477977004\n",
      "-----\n",
      "Epoch 58:\n",
      "\n",
      "Mean epoch loss:  4.006848016631937\n",
      "-----\n",
      "Epoch 59:\n",
      "\n",
      "Mean epoch loss:  3.9694712741352687\n",
      "-----\n",
      "Epoch 60:\n",
      "\n",
      "Mean epoch loss:  4.046334845997463\n",
      "-----\n",
      "Epoch 61:\n",
      "\n",
      "Mean epoch loss:  3.934405117391426\n",
      "-----\n",
      "Epoch 62:\n",
      "\n",
      "Mean epoch loss:  4.000231567944322\n",
      "-----\n",
      "Epoch 63:\n",
      "\n",
      "Mean epoch loss:  3.904181257586613\n",
      "-----\n",
      "Epoch 64:\n",
      "\n",
      "Mean epoch loss:  3.9841588757862554\n",
      "-----\n",
      "Epoch 65:\n",
      "\n",
      "Mean epoch loss:  3.992729436571353\n",
      "-----\n",
      "Epoch 66:\n",
      "\n",
      "Mean epoch loss:  3.932209506213108\n",
      "Learning rate halfed! \n",
      "\n",
      "-----\n",
      "Epoch 67:\n",
      "\n",
      "Mean epoch loss:  3.950726850010524\n",
      "-----\n",
      "Epoch 68:\n",
      "\n",
      "Mean epoch loss:  3.939766685539317\n",
      "-----\n",
      "Epoch 69:\n",
      "\n",
      "Mean epoch loss:  3.930453765058072\n",
      "-----\n",
      "Epoch 70:\n",
      "\n",
      "Mean epoch loss:  3.983886475874999\n",
      "-----\n",
      "Epoch 71:\n",
      "\n",
      "Mean epoch loss:  3.9741766051711322\n",
      "-----\n",
      "Epoch 72:\n",
      "\n",
      "Mean epoch loss:  3.953738072208155\n",
      "-----\n",
      "Epoch 73:\n",
      "\n",
      "Mean epoch loss:  3.9825828877564904\n",
      "-----\n",
      "Epoch 74:\n",
      "\n",
      "Mean epoch loss:  3.9448930281344974\n",
      "-----\n",
      "Epoch 75:\n",
      "\n",
      "Mean epoch loss:  3.9595325806430566\n",
      "-----\n",
      "Epoch 76:\n",
      "\n",
      "Mean epoch loss:  3.9625512060718\n",
      "Learning rate halfed! \n",
      "\n",
      "-----\n",
      "Epoch 77:\n",
      "\n",
      "Mean epoch loss:  3.971503386987704\n",
      "-----\n",
      "Epoch 78:\n",
      "\n",
      "Mean epoch loss:  3.9124938104754294\n",
      "-----\n",
      "Epoch 79:\n",
      "\n",
      "Mean epoch loss:  3.9128162314958663\n",
      "-----\n",
      "Epoch 80:\n",
      "\n",
      "Mean epoch loss:  3.9832230460978\n",
      "-----\n",
      "Epoch 81:\n",
      "\n",
      "Mean epoch loss:  3.921939889961314\n",
      "-----\n",
      "Epoch 82:\n",
      "\n",
      "Mean epoch loss:  3.9193061668182088\n",
      "-----\n",
      "Epoch 83:\n",
      "\n",
      "Mean epoch loss:  3.9337465506847775\n",
      "-----\n",
      "Epoch 84:\n",
      "\n",
      "Mean epoch loss:  3.920227325965311\n",
      "-----\n",
      "Epoch 85:\n",
      "\n",
      "Mean epoch loss:  3.9382582858344106\n",
      "-----\n",
      "Epoch 86:\n",
      "\n",
      "Mean epoch loss:  3.9008053463196086\n",
      "Learning rate halfed! \n",
      "\n",
      "-----\n",
      "Epoch 87:\n",
      "\n",
      "Mean epoch loss:  3.9251290704602395\n",
      "-----\n",
      "Epoch 88:\n",
      "\n",
      "Mean epoch loss:  3.901429754551326\n",
      "-----\n",
      "Epoch 89:\n",
      "\n",
      "Mean epoch loss:  3.923349546494885\n",
      "-----\n",
      "Epoch 90:\n",
      "\n",
      "Mean epoch loss:  3.9588215083719414\n",
      "-----\n",
      "Epoch 91:\n",
      "\n",
      "Mean epoch loss:  3.9129843968097293\n",
      "-----\n",
      "Epoch 92:\n",
      "\n",
      "Mean epoch loss:  3.9072660062914695\n",
      "-----\n",
      "Epoch 93:\n",
      "\n",
      "Mean epoch loss:  3.8864474586237256\n",
      "-----\n",
      "Epoch 94:\n",
      "\n",
      "Mean epoch loss:  3.9159664561815353\n",
      "-----\n",
      "Epoch 95:\n",
      "\n",
      "Mean epoch loss:  3.9293094572619855\n",
      "-----\n",
      "Epoch 96:\n",
      "\n",
      "Mean epoch loss:  3.889117672064594\n",
      "Learning rate halfed! \n",
      "\n",
      "-----\n",
      "Epoch 97:\n",
      "\n",
      "Mean epoch loss:  3.9155995461428277\n",
      "-----\n",
      "Epoch 98:\n",
      "\n",
      "Mean epoch loss:  3.905890511575146\n",
      "-----\n",
      "Epoch 99:\n",
      "\n",
      "Mean epoch loss:  3.8955417748923615\n",
      "-----\n",
      "Epoch 100:\n",
      "\n",
      "Mean epoch loss:  3.8425722411859815\n",
      "-----\n",
      "Epoch 101:\n",
      "\n",
      "Mean epoch loss:  3.8604808756124194\n",
      "-----\n",
      "Epoch 102:\n",
      "\n",
      "Mean epoch loss:  3.900812420889596\n",
      "-----\n",
      "Epoch 103:\n",
      "\n",
      "Mean epoch loss:  3.900747328161079\n",
      "-----\n",
      "Epoch 104:\n",
      "\n",
      "Mean epoch loss:  3.8815155820311786\n",
      "-----\n",
      "Epoch 105:\n",
      "\n",
      "Mean epoch loss:  3.909240375055331\n",
      "-----\n",
      "Epoch 106:\n",
      "\n",
      "Mean epoch loss:  3.8763365110504293\n",
      "Learning rate halfed! \n",
      "\n",
      "-----\n",
      "Epoch 107:\n",
      "\n",
      "Mean epoch loss:  3.9216436337087757\n",
      "-----\n",
      "Epoch 108:\n",
      "\n",
      "Mean epoch loss:  3.8607652165065303\n",
      "-----\n",
      "Epoch 109:\n",
      "\n",
      "Mean epoch loss:  3.867410048146114\n",
      "-----\n",
      "Epoch 110:\n",
      "\n",
      "Mean epoch loss:  3.888335945450257\n",
      "-----\n",
      "Epoch 111:\n",
      "\n",
      "Mean epoch loss:  3.823611921239122\n",
      "-----\n",
      "Epoch 112:\n",
      "\n",
      "Mean epoch loss:  3.8629987418094527\n",
      "-----\n",
      "Epoch 113:\n",
      "\n",
      "Mean epoch loss:  3.847122164530175\n",
      "-----\n",
      "Epoch 114:\n",
      "\n",
      "Mean epoch loss:  3.886082570129466\n",
      "-----\n",
      "Epoch 115:\n",
      "\n",
      "Mean epoch loss:  3.8637926678791223\n",
      "-----\n",
      "Epoch 116:\n",
      "\n",
      "Mean epoch loss:  3.901071664328887\n",
      "Learning rate halfed! \n",
      "\n",
      "-----\n",
      "Epoch 117:\n",
      "\n",
      "Mean epoch loss:  3.863072063321265\n",
      "-----\n",
      "Epoch 118:\n",
      "\n",
      "Mean epoch loss:  3.8708338548089856\n",
      "-----\n",
      "Epoch 119:\n",
      "\n",
      "Mean epoch loss:  3.8463670505541505\n",
      "-----\n",
      "Epoch 120:\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-21274384014f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0melement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Fe'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mphase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'BCC_A2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-50-0a8de954bfdc>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, element, phase)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-----\\nEpoch %i:\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-7db59067628e>\u001b[0m in \u001b[0;36mepoch\u001b[1;34m(net, dataloader, loss_func, optimizer)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mepoch_losses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\5_Programmcodes\\lib\\site-packages\\torch\\optim\\rprop.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[1;31m# for dir>=0 dfdx=dfdx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m                 \u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metaminus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[1;31m# update parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = ThermoNet()\n",
    "\n",
    "element = 'Fe'\n",
    "phase = ['BCC_A2']\n",
    "train(net, element, phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0c6077",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graphic_evaluation(net, element, phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cca41251",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, 'ThermoNet/Models/model_12_01_22_1535')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c691983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
