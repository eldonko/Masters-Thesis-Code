{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "598158d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Neural_Nets.ThermoNet.Development.ThermoNet import ThermoRegressionNet, ThermoDataset\n",
    "from Neural_Nets.ThermoNetActFuncs.Development.ThermoNetActFuncs import Sigmoid, Softplus, ChenSundman\n",
    "from Utils.PlotHandler.Development.PlotHandler import PlotHandler \n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from torch.optim import Rprop, Adam\n",
    "from Data_Handling.SGTEHandler.Development.SGTEHandler import SGTEHandler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51a1bb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(net: ThermoRegressionNet, dataloader, loss_func, optimizer):\n",
    "    epoch_losses = np.zeros([len(dataloader), ])\n",
    "\n",
    "    for i, (temp, g) in enumerate(dataloader):\n",
    "        temp = temp.unsqueeze(-1)\n",
    "        \n",
    "        temp /= temp.max()\n",
    "\n",
    "        # Forward pass\n",
    "        gibbs_energy = net(temp)\n",
    "        \n",
    "        scaling = 10000\n",
    "        gibbs_energy, g = gibbs_energy/scaling, g/scaling\n",
    "        \n",
    "        # Get the loss\n",
    "        loss = loss_func(gibbs_energy.float(), g.float())\n",
    "\n",
    "        # Backward pass\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 100)\n",
    "        optimizer.step()\n",
    "        epoch_losses[i] = loss\n",
    "\n",
    "    print('gibbs std: ', gibbs_energy.std(), ', gibbs mean: ', gibbs_energy.mean())\n",
    "    mean_epoch_loss = epoch_losses.mean()\n",
    "    print('Mean epoch loss: ', mean_epoch_loss)\n",
    "    return mean_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ada5d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, dataset):\n",
    "    # Hyperparameters\n",
    "    n_epochs = 100\n",
    "    lr = 0.001\n",
    "    batch_size = 128\n",
    "    std_thresh = 0.05\n",
    "\n",
    "    # Data\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = Rprop(net.parameters(), lr=lr)\n",
    "    loss_func = nn.MSELoss()\n",
    "\n",
    "    losses = []\n",
    "    \n",
    "    # Keep track of epoch where learning rate was reduced last\n",
    "    lr_reduced_last = 0\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        print('-----\\nEpoch %i:\\n' % i)\n",
    "        loss = epoch(net, dataloader, loss_func, optimizer)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Adapt learning rate if standard deviation over the last 10 epochs is below a threshold\n",
    "        if np.array(losses[-10:]).std() < std_thresh and (i - lr_reduced_last) >= 10:\n",
    "            print('Learning rate halfed! \\n')\n",
    "            lr_reduced_last = i\n",
    "            lr /= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa160953",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fe successfully selected!\n",
      "\n",
      "-----\n",
      "Epoch 0:\n",
      "\n",
      "gibbs std:  tensor(1.9929e-06, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-0.0001, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  45.78844697134836\n",
      "-----\n",
      "Epoch 1:\n",
      "\n",
      "gibbs std:  tensor(0.0015, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-0.0128, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  45.54523195539202\n",
      "-----\n",
      "Epoch 2:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\anaconda3\\envs\\5_Programmcodes\\lib\\site-packages\\torch\\nn\\modules\\loss.py:431: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\danie\\anaconda3\\envs\\5_Programmcodes\\lib\\site-packages\\torch\\nn\\modules\\loss.py:431: UserWarning: Using a target size (torch.Size([37])) that is different to the input size (torch.Size([37, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gibbs std:  tensor(0.8442, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7983, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  29.07329273223877\n",
      "-----\n",
      "Epoch 3:\n",
      "\n",
      "gibbs std:  tensor(0.7170, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.5281, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  13.125179631369454\n",
      "-----\n",
      "Epoch 4:\n",
      "\n",
      "gibbs std:  tensor(0.6521, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6127, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.758599621909005\n",
      "-----\n",
      "Epoch 5:\n",
      "\n",
      "gibbs std:  tensor(0.6740, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.5576, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.880056721823555\n",
      "-----\n",
      "Epoch 6:\n",
      "\n",
      "gibbs std:  tensor(0.6270, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.5664, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.701318400246757\n",
      "-----\n",
      "Epoch 7:\n",
      "\n",
      "gibbs std:  tensor(0.6489, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7559, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.616461549486433\n",
      "-----\n",
      "Epoch 8:\n",
      "\n",
      "gibbs std:  tensor(0.6200, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7018, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.64638362612043\n",
      "-----\n",
      "Epoch 9:\n",
      "\n",
      "gibbs std:  tensor(0.6233, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7945, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.667982986995153\n",
      "-----\n",
      "Epoch 10:\n",
      "\n",
      "gibbs std:  tensor(0.5688, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6705, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.437050819396973\n",
      "-----\n",
      "Epoch 11:\n",
      "\n",
      "gibbs std:  tensor(0.5819, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.5930, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.523946762084961\n",
      "-----\n",
      "Epoch 12:\n",
      "\n",
      "gibbs std:  tensor(0.5540, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7293, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.4939820425851\n",
      "-----\n",
      "Epoch 13:\n",
      "\n",
      "gibbs std:  tensor(0.6031, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7121, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.628873893192836\n",
      "-----\n",
      "Epoch 14:\n",
      "\n",
      "gibbs std:  tensor(0.5730, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6764, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.554765020098005\n",
      "-----\n",
      "Epoch 15:\n",
      "\n",
      "gibbs std:  tensor(0.6752, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7095, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.686347620827812\n",
      "-----\n",
      "Epoch 16:\n",
      "\n",
      "gibbs std:  tensor(0.5714, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6195, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.465729713439941\n",
      "-----\n",
      "Epoch 17:\n",
      "\n",
      "gibbs std:  tensor(0.6465, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6202, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.678690910339355\n",
      "-----\n",
      "Epoch 18:\n",
      "\n",
      "gibbs std:  tensor(0.4811, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.5920, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.360280854361397\n",
      "-----\n",
      "Epoch 19:\n",
      "\n",
      "gibbs std:  tensor(0.5083, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.8988, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.500011580330986\n",
      "-----\n",
      "Epoch 20:\n",
      "\n",
      "gibbs std:  tensor(0.5798, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.5978, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.532066208975655\n",
      "-----\n",
      "Epoch 21:\n",
      "\n",
      "gibbs std:  tensor(0.6042, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6308, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.561656815665108\n",
      "-----\n",
      "Epoch 22:\n",
      "\n",
      "gibbs std:  tensor(0.5622, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.8191, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.514356000082833\n",
      "-----\n",
      "Epoch 23:\n",
      "\n",
      "gibbs std:  tensor(0.5968, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.8179, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.601751259395055\n",
      "-----\n",
      "Epoch 24:\n",
      "\n",
      "gibbs std:  tensor(0.5411, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6722, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.514387335096087\n",
      "-----\n",
      "Epoch 25:\n",
      "\n",
      "gibbs std:  tensor(0.5891, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7494, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.633231299264091\n",
      "-----\n",
      "Epoch 26:\n",
      "\n",
      "gibbs std:  tensor(0.6339, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.5055, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.650946276528495\n",
      "-----\n",
      "Epoch 27:\n",
      "\n",
      "gibbs std:  tensor(0.5547, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7205, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.498871053968157\n",
      "-----\n",
      "Epoch 28:\n",
      "\n",
      "gibbs std:  tensor(0.5771, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7596, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.558385985238212\n",
      "-----\n",
      "Epoch 29:\n",
      "\n",
      "gibbs std:  tensor(0.6476, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6276, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.64225687299456\n",
      "-----\n",
      "Epoch 30:\n",
      "\n",
      "gibbs std:  tensor(0.6447, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6972, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.704460144042969\n",
      "-----\n",
      "Epoch 31:\n",
      "\n",
      "gibbs std:  tensor(0.5863, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.5577, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.553433009556361\n",
      "-----\n",
      "Epoch 32:\n",
      "\n",
      "gibbs std:  tensor(0.5432, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7741, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.304524149213519\n",
      "-----\n",
      "Epoch 33:\n",
      "\n",
      "gibbs std:  tensor(0.5756, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6016, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.52283866064889\n",
      "-----\n",
      "Epoch 34:\n",
      "\n",
      "gibbs std:  tensor(0.5695, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6109, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.458629471915108\n",
      "-----\n",
      "Epoch 35:\n",
      "\n",
      "gibbs std:  tensor(0.6081, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6725, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.583545889173235\n",
      "-----\n",
      "Epoch 36:\n",
      "\n",
      "gibbs std:  tensor(0.6317, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7064, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.622864655085973\n",
      "-----\n",
      "Epoch 37:\n",
      "\n",
      "gibbs std:  tensor(0.6220, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6219, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.574616500309535\n",
      "-----\n",
      "Epoch 38:\n",
      "\n",
      "gibbs std:  tensor(0.5555, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.5733, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.50388308933803\n",
      "-----\n",
      "Epoch 39:\n",
      "\n",
      "gibbs std:  tensor(0.6173, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.9402, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.650338990347725\n",
      "-----\n",
      "Epoch 40:\n",
      "\n",
      "gibbs std:  tensor(0.6201, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.8453, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.763622147696358\n",
      "-----\n",
      "Epoch 41:\n",
      "\n",
      "gibbs std:  tensor(0.6800, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.5595, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.658465794154576\n",
      "-----\n",
      "Epoch 42:\n",
      "\n",
      "gibbs std:  tensor(0.5805, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7865, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.62824283327375\n",
      "-----\n",
      "Epoch 43:\n",
      "\n",
      "gibbs std:  tensor(0.6313, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.8446, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.600703171321324\n",
      "-----\n",
      "Epoch 44:\n",
      "\n",
      "gibbs std:  tensor(0.5954, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7321, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.571227414267403\n",
      "-----\n",
      "Epoch 45:\n",
      "\n",
      "gibbs std:  tensor(0.6363, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7665, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.68845524106707\n",
      "-----\n",
      "Epoch 46:\n",
      "\n",
      "gibbs std:  tensor(0.6125, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.8306, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.570063522883824\n",
      "-----\n",
      "Epoch 47:\n",
      "\n",
      "gibbs std:  tensor(0.5765, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6782, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.527769429343087\n",
      "-----\n",
      "Epoch 48:\n",
      "\n",
      "gibbs std:  tensor(0.5961, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6744, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.526671409606934\n",
      "-----\n",
      "Epoch 49:\n",
      "\n",
      "gibbs std:  tensor(0.6236, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6857, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.489761420658656\n",
      "-----\n",
      "Epoch 50:\n",
      "\n",
      "gibbs std:  tensor(0.6347, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6456, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.645764691489083\n",
      "-----\n",
      "Epoch 51:\n",
      "\n",
      "gibbs std:  tensor(0.5058, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6359, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.373331206185478\n",
      "-----\n",
      "Epoch 52:\n",
      "\n",
      "gibbs std:  tensor(0.5956, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.5313, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.544352395193917\n",
      "-----\n",
      "Epoch 53:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gibbs std:  tensor(0.5662, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6778, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.46774080821446\n",
      "-----\n",
      "Epoch 54:\n",
      "\n",
      "gibbs std:  tensor(0.6216, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.4808, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.614516735076904\n",
      "-----\n",
      "Epoch 55:\n",
      "\n",
      "gibbs std:  tensor(0.5866, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6821, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.601058687482562\n",
      "-----\n",
      "Epoch 56:\n",
      "\n",
      "gibbs std:  tensor(0.6423, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7014, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.713847705296107\n",
      "-----\n",
      "Epoch 57:\n",
      "\n",
      "gibbs std:  tensor(0.6119, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7283, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.63036332811628\n",
      "-----\n",
      "Epoch 58:\n",
      "\n",
      "gibbs std:  tensor(0.6521, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.8043, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.692356995173864\n",
      "-----\n",
      "Epoch 59:\n",
      "\n",
      "gibbs std:  tensor(0.6586, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7592, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.733216762542725\n",
      "-----\n",
      "Epoch 60:\n",
      "\n",
      "gibbs std:  tensor(0.6176, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7676, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.67877653666905\n",
      "-----\n",
      "Epoch 61:\n",
      "\n",
      "gibbs std:  tensor(0.6582, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6004, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.677581037793841\n",
      "-----\n",
      "Epoch 62:\n",
      "\n",
      "gibbs std:  tensor(0.6316, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6911, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.654141562325615\n",
      "-----\n",
      "Epoch 63:\n",
      "\n",
      "gibbs std:  tensor(0.6697, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6855, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.78711189542498\n",
      "-----\n",
      "Epoch 64:\n",
      "\n",
      "gibbs std:  tensor(0.6852, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6445, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.83269282749721\n",
      "-----\n",
      "Epoch 65:\n",
      "\n",
      "gibbs std:  tensor(0.6174, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6155, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.603582790919713\n",
      "-----\n",
      "Epoch 66:\n",
      "\n",
      "gibbs std:  tensor(0.6540, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6509, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.726403304508754\n",
      "-----\n",
      "Epoch 67:\n",
      "\n",
      "gibbs std:  tensor(0.5931, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6320, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.54711641584124\n",
      "-----\n",
      "Epoch 68:\n",
      "\n",
      "gibbs std:  tensor(0.5863, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6600, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.55543313707624\n",
      "-----\n",
      "Epoch 69:\n",
      "\n",
      "gibbs std:  tensor(0.6476, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6696, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.638275759560722\n",
      "-----\n",
      "Epoch 70:\n",
      "\n",
      "gibbs std:  tensor(0.6245, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.5660, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.590990952083043\n",
      "-----\n",
      "Epoch 71:\n",
      "\n",
      "gibbs std:  tensor(0.6831, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7485, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.722826480865479\n",
      "-----\n",
      "Epoch 72:\n",
      "\n",
      "gibbs std:  tensor(0.6205, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6718, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.585447720118932\n",
      "-----\n",
      "Epoch 73:\n",
      "\n",
      "gibbs std:  tensor(0.5443, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.8917, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.548803465706962\n",
      "-----\n",
      "Epoch 74:\n",
      "\n",
      "gibbs std:  tensor(0.5824, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6032, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.523892402648926\n",
      "-----\n",
      "Epoch 75:\n",
      "\n",
      "gibbs std:  tensor(0.5641, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7804, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.582259791237968\n",
      "-----\n",
      "Epoch 76:\n",
      "\n",
      "gibbs std:  tensor(0.6440, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7365, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.661924362182617\n",
      "-----\n",
      "Epoch 77:\n",
      "\n",
      "gibbs std:  tensor(0.6140, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6063, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.596941607339042\n",
      "-----\n",
      "Epoch 78:\n",
      "\n",
      "gibbs std:  tensor(0.5916, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7076, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.57722520828247\n",
      "-----\n",
      "Epoch 79:\n",
      "\n",
      "gibbs std:  tensor(0.5907, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6570, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.622974940708705\n",
      "-----\n",
      "Epoch 80:\n",
      "\n",
      "gibbs std:  tensor(0.5659, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7041, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.543542453220912\n",
      "-----\n",
      "Epoch 81:\n",
      "\n",
      "gibbs std:  tensor(0.6293, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6644, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.644402640206474\n",
      "Learning rate halfed! \n",
      "\n",
      "-----\n",
      "Epoch 82:\n",
      "\n",
      "gibbs std:  tensor(0.6088, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7476, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.611603941236224\n",
      "-----\n",
      "Epoch 83:\n",
      "\n",
      "gibbs std:  tensor(0.6071, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6593, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.557158810751778\n",
      "-----\n",
      "Epoch 84:\n",
      "\n",
      "gibbs std:  tensor(0.5935, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6505, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.484530448913574\n",
      "-----\n",
      "Epoch 85:\n",
      "\n",
      "gibbs std:  tensor(0.6345, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7304, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.578712803976876\n",
      "-----\n",
      "Epoch 86:\n",
      "\n",
      "gibbs std:  tensor(0.6728, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6818, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.752263614109584\n",
      "-----\n",
      "Epoch 87:\n",
      "\n",
      "gibbs std:  tensor(0.5854, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.8094, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.56908791405814\n",
      "-----\n",
      "Epoch 88:\n",
      "\n",
      "gibbs std:  tensor(0.6729, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6923, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.744715281895228\n",
      "-----\n",
      "Epoch 89:\n",
      "\n",
      "gibbs std:  tensor(0.6531, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7144, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.717477457863945\n",
      "-----\n",
      "Epoch 90:\n",
      "\n",
      "gibbs std:  tensor(0.6133, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7531, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.589299065726143\n",
      "-----\n",
      "Epoch 91:\n",
      "\n",
      "gibbs std:  tensor(0.6385, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6310, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.660107612609863\n",
      "-----\n",
      "Epoch 92:\n",
      "\n",
      "gibbs std:  tensor(0.5852, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7682, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.49828018460955\n",
      "-----\n",
      "Epoch 93:\n",
      "\n",
      "gibbs std:  tensor(0.6478, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6882, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.698861054011754\n",
      "-----\n",
      "Epoch 94:\n",
      "\n",
      "gibbs std:  tensor(0.6760, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7349, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.757317611149379\n",
      "-----\n",
      "Epoch 95:\n",
      "\n",
      "gibbs std:  tensor(0.6305, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.4596, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.617529800959996\n",
      "-----\n",
      "Epoch 96:\n",
      "\n",
      "gibbs std:  tensor(0.5576, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.6261, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.460205078125\n",
      "-----\n",
      "Epoch 97:\n",
      "\n",
      "gibbs std:  tensor(0.5510, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7408, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.470831189836774\n",
      "-----\n",
      "Epoch 98:\n",
      "\n",
      "gibbs std:  tensor(0.6428, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7548, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.676599161965507\n",
      "-----\n",
      "Epoch 99:\n",
      "\n",
      "gibbs std:  tensor(0.5402, grad_fn=<StdBackward0>) , gibbs mean:  tensor(-5.7083, grad_fn=<MeanBackward0>)\n",
      "Mean epoch loss:  12.44692884172712\n"
     ]
    }
   ],
   "source": [
    "net = ThermoRegressionNet(hidden_layers=2, hidden_dim=16, act_func=nn.ReLU())\n",
    "\n",
    "element = 'Fe'\n",
    "phase = ['BCC_A2']\n",
    "dataset = ThermoDataset(element, phase)\n",
    "\n",
    "train(net, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56ae29a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ph = PlotHandler()\n",
    "\n",
    "ph.properties_temp(net, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
